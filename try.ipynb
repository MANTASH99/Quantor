{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'b id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Quantor/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdated_file.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Check for NaN or Inf values in the original data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Group by the base ID and s                               um over the columns (excluding 'id' and 'group')\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Quantor/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Desktop/Quantor/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "import numpy as np\n",
    "df =pd.read_csv('updated_file.csv', delimiter='\\t', encoding='UTF-8')\n",
    "# Check for NaN or Inf values in the original data\n",
    "\n",
    "\n",
    "df['group'] = df['b id'].str.split('_').str[1]\n",
    "print(df['group'])\n",
    "                    \n",
    "# Group by the base ID and s                               um over the columns (excluding 'id' and 'group')\n",
    "grouped_sum = df.groupby('group').sum()\n",
    "print(grouped_sum)\n",
    "grouped_sum.to_csv('Summed_new_features.csv', index='False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     group                                               b id  token  sent  \\\n",
      "0  s1a-001  icesing_s1a-001_1_1icesing_s1a-001_1_2icesing_...   1939   209   \n",
      "1  s1a-002  icesing_s1a-002_1_1icesing_s1a-002_1_2icesing_...   1969   334   \n",
      "2  s1a-003  icesing_s1a-003_1_1icesing_s1a-003_1_2icesing_...   1936   291   \n",
      "3  s1a-004  icesing_s1a-004_1_1icesing_s1a-004_1_2icesing_...   1999   283   \n",
      "4  s1a-005  icesing_s1a-005_1_2icesing_s1a-005_1_3icesing_...   1939   353   \n",
      "\n",
      "   word   ld  word_S  lexical_density       nn_W       np_W  ...  time_adv_W  \\\n",
      "0  1939  811  1939.0        88.618333  33.318585   4.105490  ...    2.708864   \n",
      "1  1969  843  1969.0       138.017368  45.851733  12.823748  ...    3.543943   \n",
      "2  1936  849  1936.0       124.975649  43.509770  13.976375  ...    4.157288   \n",
      "3  1999  846  1999.0       117.221861  33.895172   8.507669  ...    3.928936   \n",
      "4  1939  823  1939.0       144.796046  53.232122  15.264584  ...    3.199759   \n",
      "\n",
      "   nom_initial_S  prep_initial_S  adv_initial_S  text_initial_S  wh_initial_S  \\\n",
      "0           81.0              10           30.0              10           9.0   \n",
      "1          151.0               6           26.0              20          19.0   \n",
      "2          132.0               7           26.0              22          13.0   \n",
      "3          132.0               3           42.0              17          21.0   \n",
      "4          128.0              12           36.0              23          24.0   \n",
      "\n",
      "   disc_initial_S  nonfin_initial_S  subord_initial_S  verb_initial_S  \n",
      "0              20                 5                20            13.0  \n",
      "1              66                 6                17            17.0  \n",
      "2              48                15                19            13.0  \n",
      "3              25                 7                14            13.0  \n",
      "4              60                15                29            16.0  \n",
      "\n",
      "[5 rows x 49 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_frame = pd.read_csv('Summed_new_features.csv' ,  delimiter=',')\n",
    "\n",
    "print(data_frame.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "     group                                                 id  n_token  \\\n",
      "0  s1a-001  icesing_s1a-001_1_1icesing_s1a-001_1_2icesing_...     1939   \n",
      "1  s1a-002  icesing_s1a-002_1_1icesing_s1a-002_1_2icesing_...     1969   \n",
      "2  s1a-003  icesing_s1a-003_1_1icesing_s1a-003_1_2icesing_...     1936   \n",
      "3  s1a-004  icesing_s1a-004_1_1icesing_s1a-004_1_2icesing_...     1999   \n",
      "4  s1a-005  icesing_s1a-005_1_2icesing_s1a-005_1_3icesing_...     1939   \n",
      "\n",
      "   n_sent  n_word   ld   nn  np  nom  neo  ...  numbertheme  pptheme  \\\n",
      "0     209    1939  811  269  33   32    4  ...            9       10   \n",
      "1     334    1969  843  270  49   35    9  ...            2        6   \n",
      "2     291    1936  849  279  71   30    5  ...            7        7   \n",
      "3     283    1999  846  256  36   30    5  ...           11        3   \n",
      "4     353    1939  823  280  51   48    4  ...           19       12   \n",
      "\n",
      "   advtheme  cctheme  whtheme  disctheme  totheme  cstheme  subordtheme  \\\n",
      "0        30       10        9         20        5       15           20   \n",
      "1        26       20       19         66        6       11           17   \n",
      "2        26       22       13         48       15        4           19   \n",
      "3        42       17       21         25        7        7           14   \n",
      "4        36       23       24         60       15       14           29   \n",
      "\n",
      "   verbtheme  \n",
      "0         13  \n",
      "1         17  \n",
      "2         13  \n",
      "3         13  \n",
      "4         16  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "Index(['group', 'id', 'n_token', 'n_sent', 'n_word', 'ld', 'nn', 'np', 'nom',\n",
      "       'neo', 'poss', 'pronoun', 'p1', 'p2', 'p3', 'pit', 'pospers1',\n",
      "       'pospers2', 'pospers3', 'adj', 'atadj', 'prep', 'fin', 'past', 'will',\n",
      "       'vm', 'v', 'inf', 'pass', 'coord', 'subord', 'interr', 'imper', 'title',\n",
      "       'salutgreet', 'rl', 'rt', 'nptheme', 'numbertheme', 'pptheme',\n",
      "       'advtheme', 'cctheme', 'whtheme', 'disctheme', 'totheme', 'cstheme',\n",
      "       'subordtheme', 'verbtheme'],\n",
      "      dtype='object')\n",
      "Normalized data saved to normalized_TEXTS.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Summed.csv', delimiter=';')\n",
    "\n",
    "# Print basic info for debugging\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "\n",
    "# Step 1: Identify all numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Step 2: Normalize all numeric columns by 'n_token'\n",
    "for col in numeric_columns:\n",
    "    # Divide each column by its row's 'n_token' value\n",
    "    df[col] = df[col] / df['n_token']\n",
    "\n",
    "# Step 3: After normalization, set the 'n_token' column values to 1\n",
    "df['n_token'] = 1\n",
    "\n",
    "# Step 4: Save the normalized DataFrame to a new CSV file\n",
    "output_file = 'normalized_TEXTS.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Normalized data saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
